{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from mlstm_kernels.components.conv import CausalConv1d, CausalConv1dConfig\n",
    "from mlstm_kernels.mlstm import get_available_mlstm_kernels, get_mlstm_kernel\n",
    "from mlstm_kernels.mlstm.backend_module import mLSTMBackend, mLSTMBackendConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recurrent_step--step_torch_autograd',\n",
       " 'recurrent_step--step_triton',\n",
       " 'recurrent_step--step_fused_triton',\n",
       " 'recurrent_sequence--sequence_torch_autograd',\n",
       " 'chunkwise--torch_autograd',\n",
       " 'chunkwise--torch_ownbw',\n",
       " 'chunkwise--max_triton',\n",
       " 'chunkwise--max_triton_v1',\n",
       " 'chunkwise--max_triton_v2',\n",
       " 'chunkwise--max_triton_v3',\n",
       " 'chunkwise--triton',\n",
       " 'chunkwise--stable_triton',\n",
       " 'parallel--torch_autograd',\n",
       " 'parallel--torch_ownbw',\n",
       " 'parallel--stable_torch_autograd',\n",
       " 'parallel--stable_torch_ownbw',\n",
       " 'parallel--triton']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_mlstm_kernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlstm_backend_module = mLSTMBackend(mLSTMBackendConfig(kernel_name=\"chunkwise--torch_ownbw\", chunk_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 1  # seq len\n",
    "B = 1  # batch size\n",
    "NH = 1  # num heads\n",
    "DHQK = 1024  # dim per head\n",
    "DHV = 1024  # dim per head\n",
    "\n",
    "EPS = 0.0\n",
    "\n",
    "vecI_offset = 0.0\n",
    "vecF_offset = 6.0\n",
    "DTYPE = torch.bfloat16  # torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "matQ = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "matK = torch.randn((B, NH, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "matV = torch.randn((B, NH, S, DHV), dtype=DTYPE, device=DEVICE)\n",
    "# vecI = 0.00001 * torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "# vecF = -30. + torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "vecI = vecI_offset + torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)\n",
    "vecF = vecF_offset + torch.randn((B, NH, S), dtype=DTYPE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module      FLOP    % Total\n",
      "--------  ------  ---------\n",
      "Global         0         0%\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Sequence length 1 is not divisible by chunk size 128.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FlopCounterMode():\n\u001b[0;32m----> 2\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmlstm_backend_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvecI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvecF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs-gpu/xlstm/miniforge3/envs/xlstmpt240cu124/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs-gpu/xlstm/miniforge3/envs/xlstmpt240cu124/lib/python3.11/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/nfs-gpu/users_home/beck/dev_repos/xlstm-dev_dev/mlstm_kernels/notebooks/flop_counting/../../mlstm_kernels/mlstm/backend_module.py:52\u001b[0m, in \u001b[0;36mmLSTMBackend.forward\u001b[0;34m(self, q, k, v, i, f, c_initial, n_initial, m_initial, return_last_states)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     39\u001b[0m     q: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]\n\u001b[1;32m     51\u001b[0m ):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mm_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_last_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_last_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast_kernel_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast_kernel_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs-gpu/users_home/beck/dev_repos/xlstm-dev_dev/mlstm_kernels/notebooks/flop_counting/../../mlstm_kernels/mlstm/chunkwise/torch_fwbw/torch_fwbw.py:281\u001b[0m, in \u001b[0;36mmlstm_chunkwise_torch_ownbw\u001b[0;34m(q, k, v, i, f, c_initial, n_initial, m_initial, return_last_states, eps, chunk_size, autocast_kernel_dtype)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmlstm_chunkwise_torch_ownbw\u001b[39m(\n\u001b[1;32m    267\u001b[0m     q: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    268\u001b[0m     k: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     autocast_kernel_dtype: torch\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m    279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    280\u001b[0m     _mlstm_chunkwise_fwbw \u001b[38;5;241m=\u001b[39m _get_chunkwise_fwbw_kernel(autocast_kernel_dtype)\n\u001b[0;32m--> 281\u001b[0m     matH_out, matC_last, vecN_last, scaM_last \u001b[38;5;241m=\u001b[39m \u001b[43m_mlstm_chunkwise_fwbw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mm_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_last_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_last_states:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m matH_out, (matC_last, vecN_last, scaM_last)\n",
      "File \u001b[0;32m/nfs-gpu/xlstm/miniforge3/envs/xlstmpt240cu124/lib/python3.11/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/nfs-gpu/xlstm/miniforge3/envs/xlstmpt240cu124/lib/python3.11/site-packages/torch/amp/autocast_mode.py:466\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fwd(\n\u001b[1;32m    462\u001b[0m             \u001b[38;5;241m*\u001b[39m_cast(args, device_type, cast_inputs),\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_cast(kwargs, device_type, cast_inputs),\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs-gpu/users_home/beck/dev_repos/xlstm-dev_dev/mlstm_kernels/notebooks/flop_counting/../../mlstm_kernels/torch.utils.py:17\u001b[0m, in \u001b[0;36mcontiguous.<locals>.wrapper\u001b[0;34m(ctx, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs-gpu/users_home/beck/dev_repos/xlstm-dev_dev/mlstm_kernels/notebooks/flop_counting/../../mlstm_kernels/mlstm/chunkwise/torch_fwbw/torch_fwbw.py:58\u001b[0m, in \u001b[0;36m_mlstm_chunkwise_fwbw_generator.<locals>._mlstm_chunkwise_fwbw.forward\u001b[0;34m(ctx, matQ, matK, matV, vecI, vecF, matC_initial, vecN_initial, scaM_initial, qk_scale, return_last_states, RECOMPUTE_STATES_IN_BW, CHUNK_SIZE, EPS)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qk_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     qk_scale \u001b[38;5;241m=\u001b[39m DHQK\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 58\u001b[0m matH_out, vecN_out, vecM_out, last_states, all_states \u001b[38;5;241m=\u001b[39m \u001b[43m_mlstm_chunkwise_fw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvecI\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvecI\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvecF\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvecF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatC_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatC_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvecN_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvecN_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaM_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaM_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_last_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_last_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_all_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRECOMPUTE_STATES_IN_BW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_last_states:\n\u001b[1;32m     75\u001b[0m     (matC_last, vecN_last, scaM_last) \u001b[38;5;241m=\u001b[39m last_states\n",
      "File \u001b[0;32m/nfs-gpu/users_home/beck/dev_repos/xlstm-dev_dev/mlstm_kernels/notebooks/flop_counting/../../mlstm_kernels/mlstm/chunkwise/torch_fwbw/_torch_fw.py:266\u001b[0m, in \u001b[0;36m_mlstm_chunkwise_fw\u001b[0;34m(matQ, matK, matV, vecI, vecF, matC_initial, vecN_initial, scaM_initial, qk_scale, return_last_states, return_all_states, CHUNK_SIZE, EPS)\u001b[0m\n\u001b[1;32m    263\u001b[0m B, NH, S, DHQK \u001b[38;5;241m=\u001b[39m matQ\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    264\u001b[0m DHV \u001b[38;5;241m=\u001b[39m matV\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 266\u001b[0m     S \u001b[38;5;241m%\u001b[39m CHUNK_SIZE \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    267\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not divisible by chunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHUNK_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m NC \u001b[38;5;241m=\u001b[39m S \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m CHUNK_SIZE\n\u001b[1;32m    270\u001b[0m vecI \u001b[38;5;241m=\u001b[39m rearrange(vecI, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb nh (nc l) -> b nh nc l\u001b[39m\u001b[38;5;124m\"\u001b[39m, l\u001b[38;5;241m=\u001b[39mCHUNK_SIZE)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Sequence length 1 is not divisible by chunk size 128."
     ]
    }
   ],
   "source": [
    "with FlopCounterMode():\n",
    "    out = mlstm_backend_module(matQ, matK, matV, vecI, vecF, EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalConv1d(\n",
       "  (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = 10  # seq len\n",
    "B = 1  # batch size\n",
    "DHQK = 1  # dim per head\n",
    "kernel_size = 4\n",
    "z_c = torch.randn((B, S, DHQK), dtype=DTYPE, device=DEVICE)\n",
    "conv1d = CausalConv1d(CausalConv1dConfig(feature_dim=DHQK, kernel_size=kernel_size)).to(dtype=DTYPE, device=DEVICE)\n",
    "conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                  FLOP    % Total\n",
      "--------------------  ------  ---------\n",
      "CausalConv1d             104    100.00%\n",
      " - aten.convolution      104    100.00%\n",
      " CausalConv1d.conv       104    100.00%\n",
      "  - aten.convolution     104    100.00%\n"
     ]
    }
   ],
   "source": [
    "with FlopCounterMode():\n",
    "    out_conv = conv1d(z_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_conv = 2 * kernel_size * (S + kernel_size - 1) * DHQK\n",
    "flop_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstmpt240cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
